{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In iterative algorithms such as gradient descent we are concerned with convergence. Here is a summary of the convergence concept:\n",
    "* Compute an error term, call it $e_k = ||\\hat{r}_k - r||$ where $\\hat{r}_k$ is the estimate at each iteration and $r$ is the true value\n",
    "* The goal is for $\\displaystyle\\lim_{k\\to\\infty}e_k = 0$\n",
    "* Comparing $e_k$ to 0 directly, as in, is $e_k=0$? is a bad idea due to floating point representation of numbers\n",
    "* Instead we will compare $e_k$ to some tolerance $\\epsilon$ (recall $\\epsilon-\\delta$ proofs!) and terminate the algorithm when $e_k = ||\\hat{r}_k - r||<\\epsilon$\n",
    "* What if, as is often the case, $r$ is unknown? This question will be considered later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The problem with $==$ and floating-point considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = 1/10+1/10+1/10\n",
    "xtrue = 3/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's Method Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evec, its, root = newton(4,f,df,1e-8)\n",
    "print('The root is {} after {} iterations with error = {:2.4e}.'.format(root,its,evec[-1]))\n",
    "plt.semilogy(np.arange(1,len(evec)),evec[1:],'ro-')\n",
    "plt.xticks(np.arange(1,len(evec)))\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evec, its, root = newton(4,df,df2,1e-8)\n",
    "print('The root is {} after {} iterations with error = {:2.4e}.'.format(root,its,evec[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient Descent in 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent1D(x, eta, tol = 1e-8, nmax = 100):\n",
    "    # *************************************************************\n",
    "    # gradient_descent1D (GD)\n",
    "    #\n",
    "    # Internal parameters:\n",
    "    #      iterror := abs(x_current - x_previous)\n",
    "    #      k := number of iterations\n",
    "    #      errvec := array of iterrors (history)\n",
    "    #\n",
    "    # Arguments:\n",
    "    #      x := initial guess\n",
    "    #      eta := step-size (learning rate)\n",
    "    #      tol := tolerance for iteration error\n",
    "    #      nmax := max number of iterations allowed by while loop\n",
    "    #\n",
    "    # Output:\n",
    "    #      x, k, errvec (as defined above)\n",
    "    # *************************************************************\n",
    "    \n",
    "    # Initialize the internal parameters\n",
    "    iterror = tol+1\n",
    "    k = 0\n",
    "    errvec = []\n",
    "    # Implement the GD algorithm\n",
    "\n",
    "    # Return output\n",
    "    return x, k, errvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Guess\n",
    "np.random.seed(0)\n",
    "x0 = 10*np.random.rand(1)[0]\n",
    "print(\"x0 = {:2.2f}\".format(x0))\n",
    "# Set the simulation parameters\n",
    "eta, nmax, tol = 0.25, 100, 1e-4\n",
    "# Run GD and print output\n",
    "xmin, n, errvec = gradient_descent1D(x0,eta,tol)\n",
    "print(\"Step-size: {:}, tolerance: {:}\".format(eta,tol))\n",
    "print(\"After {:d} iterations the min is {:2.4f}\".format(n,xmin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,f(x),'b',xmin,f(xmin),'ro')\n",
    "plt.xlabel('$x$',size=14)\n",
    "plt.ylabel('$f(x)$',size=14)\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(np.arange(len(errvec))+1,errvec,'ro-')\n",
    "plt.xlabel('iterations',size=15)\n",
    "plt.ylabel('error',size=15)\n",
    "plt.axis([1,len(errvec),0,max(errvec)])\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descentLR(x, y, eta, tol=1e-4, itmax = 100):\n",
    "    # *************************************************************\n",
    "    # gradient_descentLR\n",
    "    #\n",
    "    # Internal parameters:\n",
    "    #      iterror := abs(x_current - x_previous)\n",
    "    #      epochs := number of iterations\n",
    "    #      errvec := array of iterrors (history)\n",
    "    #      w := weight vector corresponding to each feature\n",
    "    #\n",
    "    # Arguments:\n",
    "    #      x := feature set\n",
    "    #      y := data\n",
    "    #      eta := step-size\n",
    "    #      tol := convergence tolerance\n",
    "    #      itmax := max number of iterations allowed by while loop\n",
    "    #\n",
    "    # Output:\n",
    "    #      W := weight vector corresponding to each feature\n",
    "    #      epochs := number of iterations to reach convergence\n",
    "    #      errvec\n",
    "    # *************************************************************\n",
    "    \n",
    "    # Initialize the internal parameters\n",
    "    iterror, epochs = tol+1, 1\n",
    "    errvec = []\n",
    "    # Initialize the weights with some random vector\n",
    "    W = np.random.rand(2)\n",
    "    X = np.c_[np.ones(len(x)),x]\n",
    "    # Implement the GD algorithm\n",
    "    while (iterror > tol) and (epochs < itmax):       \n",
    "        W0 = W.copy()\n",
    "        pred = \n",
    "        error = \n",
    "        grad = \n",
    "        W -= eta*grad\n",
    "        \n",
    "        iterror = np.sqrt((W-W0).dot(W-W0))\n",
    "        errvec.append(iterror)\n",
    "        epochs += 1\n",
    "    # Return output\n",
    "    return W, epochs-1, errvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(50)\n",
    "y = 1.5*x + 1.1 + np.random.randn(len(x))*0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Display Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the simulation parameters\n",
    "eta = 3e-2\n",
    "itmax = 5000\n",
    "tol = 1e-4\n",
    "# Run GD\n",
    "w, epochs, errvec = gradient_descentLR(x,y,eta,tol,itmax)\n",
    "# ******************************************************************\n",
    "# Display the results\n",
    "# ******************************************************************\n",
    "print(\"Step-size: {:}, tolerance: {:}\".format(eta,tol))\n",
    "print(\"After {:d} iterations the min is {:}\".format(len(errvec),w))\n",
    "plt.plot(np.arange(len(errvec))+1,errvec)\n",
    "plt.xlabel('iterations',size=15)\n",
    "plt.ylabel('error',size=15)\n",
    "plt.axis([1,len(errvec),0,max(errvec)])\n",
    "plt.grid(True);\n",
    "plt.figure()\n",
    "plt.plot(x,y,'bo',x,w[0] + w[1]*x,'r')\n",
    "plt.xlabel('$x$',size=25)\n",
    "plt.ylabel('$y$',size=25)\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
